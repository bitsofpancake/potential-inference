<!doctype html>
<html>
	<head>
		<title>Inferring the Mass Distribution of the Milky Way - Casey Chu</title>
		<link href="poster.css" type="text/css" rel="stylesheet" />
		<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.3.0/katex.min.css" type="text/css" rel="stylesheet" />
	</head>
	<body>
		<header>
			<h1>Inferring the Mass Distribution of the Milky Way</h1>
			<div class="author">Casey Chu<sup>1,2</sup>, Yoram Lithwick<sup>1</sup>, Fabio Antonini<sup>1</sup></div>
			<div class="insitutions"><sup>1</sup>Northwestern University, <sup>2</sup>Harvey Mudd College</div>
		</header>
		
		<main>			
			<article>
				<h2>How do we study something we can't see?</h2>

				<p>One way to study dark matter is to observe its gravitational effect on visible objects: stars. And we're in luck, because the spacecraft <i>Gaia</i> is currently recording the position and velocity of one billion stars in the Milky Way.<sup>1</sup> These movements contain a large amount of statistical information about the gravitational potential they are traveling in. If we knew this gravitational potential, then we could find the distribution of matter in the Milky Way, using Poisson's equation.<sup>2</sup></p>
				
				<p>We've created a simple, numerical algorithm to extract the gravitational potential from the data. Given a set of observed positions and velocities of particles (stars), we are able to infer the potential in which they are traveling.</p>
				<!--
				<ol class="citations">
					<li>Perryman et al. 2001. Journal whose Name I'll Look Up.</span></li>
					<li>Binney &amp; Tremaine 2008. Journal whose Name I'll Look Up.</span></li>
				</ol-->
			</article>
		
			<article>
				<h2>The fundamental assumption: phase-mixing</h2>
				
				<p>The algorithm we developed relies on the system being <em>phase-mixed</em>. Figure 1 depicts the phase-mixing of 200 particles that are initially close together in phase space, evolving in a potential of <var>\Phi(x) = \frac{1}{2}|x|^\alpha</var> for <var>\alpha = 1.5</var>.</p>
				
				<figure>
					<!--video controls>
						<source src="images/phase_mixing.webm" type="video/webm" />
					</video-->
					<img src="images/evolution_0.svg" />
					<img src="images/evolution_1.svg" />
					<img src="images/evolution_2.svg" />
					<img src="images/evolution_3.svg" />
					<img src="images/evolution_4.svg" />
					<img src="images/evolution_5.svg" />
					<figcaption>The phase-mixing of 200 particles. The arrows<br />trace the phase-space flow generated by the potential.</figcaption>
				</figure>
				
				<p>Notice that as time progresses, the particles approach a configuration that is macroscopically steady-state, or phase-mixed.</p>

				<p>One important property of phase-mixing is that the <em>same</em> set of particles evolve to <em>different</em> steady-state "shapes" when they are evolved under different potentials (e.g., different values of <var>\alpha</var>).</p>
				<!--
				<p>Figure 2 depicts an important property of phase-mixing: the <em>same</em> set of particles evolve to <em>different</em> steady-state "shapes" when they are evolved under different potentials (e.g., different values of <var>\alpha</var>).</p>
				
				<figure>
					<img src="images/evolved_1.5.svg" />
					<img src="images/evolved_1.8.svg" />
					<img src="images/evolved_2.5.svg" />
					<figcaption>Steady-state configurations for different potentials.</figcaption>
				</figure>
				
				<p>This property, that different potentials produce different steady-state configurations, is crucial to our method.</p>
-->
			</article>
			
			<article>
				<h2>The insight behind the inference algorithm</h2>
				
				<!--figure class="float" style="width: 360px; margin-top: 0.75em">
					<img src="images/unknown.svg" />
					<figcaption>A sample data set: particles observed from an unknown potential.[consider strongly getting rid of]</figcaption>
				</figure-->
				
				<!--p>Suppose that we are given the positions and velocities of particles taken from an unknown potential. Our aim is to guess which potential these particles come from.</p-->
				<p>Let's assume that we know that the particles we observe are in a steady-state configuration.</p>
				
			<ul>
				<li>If we evolve the particles under the <em>correct</em> potential, they will remain in the <em>same, steady-state configuration</em>.</li>
				<li>Conversely, if we evolve them under an <em>incorrect</em> potential, they will evolve towards a <em>different configuration</em>.</li>
			</ul>
			
				<p>Using this insight, one algorithm to infer the correct potential from a set of observations is to simply to guess a bunch of different potentials, evolve the observed particles under each of these trial potentials, and see which potential best preserves the original configuration.</p>
				
				<p>We demonstrate this in Figure 3, where a set of particles, drawn from an unknown potential, is evolved under different trial potentials.</p>
				
				<figure>
					<!--
					<video controls loop style="width: 100%">
						<source src="images/evolution.webm" type="video/webm" />
					</video>-->
					<img src="images/comparison0_1.5.svg" />
					<img src="images/comparison0_1.8.svg" />
					<img src="images/comparison0_2.5.svg" />
					<img src="images/comparison1_1.5.svg" />
					<img src="images/comparison1_1.8.svg" />
					<img src="images/comparison1_2.5.svg" />
					<figcaption>The evolution of observed particles under different potentials.</figcaption>
				</figure>
				
				<p>In this simple example, the particles most likely come from an <var>\alpha = 1.5</var> potential, because the configuration of the particles evolved under that potential most resembles the original, observed distribution.</p>
			</article>
			
			<!--article>
				<h2>Mathematical derivation</h2>
				
				<p>Rather than simply choosing the best potential by eye, let us quantify how good a trial potential is, i.e. how close an evolved configuration is to the observed particles. Let's assign a "likelihood" score to each trial potential, with higher scores corresponding to closer configurations.</p>
				
				<p>
					First, we define a function that encodes the distribution of our observed particles after having been evolved for a time <var>t</var> under a potential <var>\Phi</var>. We use a technique called <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank">kernel density estimation</a> to approximate the phase-space density at time <var>t</var>; define a function <var>f</var> to be
					<code>f(x,v;t \,|\, \Phi) = \sum_i K(x_i(t) - x)\, K(v_i(t) - v),</code>
					where <var>x_i(t)</var> and <var>v_i(t)</var> denote the position and velocity of the <var>i</var>th particle evolved to time <var>t</var> under the potential <var>\Phi</var>, and <var>K(\cdot)</var> is a <em>kernel</em>, a function that is non-zero only around <var>0</var>. For example, a typical kernel is a Gaussian centered at <var>0</var>. The result is that this function <var>f(x,v;t\,|\,\Phi)</var> gives an approximation to the phase-space density of particles at <var>x</var> and <var>v</var> and time <var>t</var>.
				</p>
				
				<p>
					Next, we define a time-averaged version of <var>f</var>:
					<code>f(x,v \,|\, \Phi) = \frac{1}{T}\int_0^T f(x,v;t\,|\,\Phi) \,dt.</code>
					This approximates the steady-state phase-space density of particles evolved under a potential <var>\Phi</var>.
				</p>
				
				<p>If <var>f</var> is properly normalized, we can treat this density function as a probability distribution function. Then the probability of observing the positions and velocities we observed assuming a particular potential is
					<code>
						\Pr(x_1,v_1,\ldots,x_n,v_n\,|\,\Phi) = \prod_i f(x_i, v_i \,|\, \Phi),
					</code>
					assuming that the particles were drawn independently.
				</p>
				
				<p>This allows us to assign a score to different trial potentials, based on our observed data, using <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank">Bayes' theorem</a>:
					<code>
						\Pr(\Phi \,|\, x_1,v_1,\ldots,x_n,v_n) \propto \Pr(\Phi)\,\Pr(x_1,v_1,\ldots,x_n,v_n\,|\,\Phi).
					</code>
					The factor of <var>\Pr(\Phi)</var> is called the <em>prior distribution</em> of <var>\Phi</var>, which is negligible in many cases. This final function is called the <em>posterior distribution</em> of <var>\Phi</var> (or simply, the posterior), and it satisfies our original goal: it's greatest for the most probable trial potentials!
				</p>
			</article-->
			<article>
				<h2>The likelihood function</h2>
				<p>Instead of choosing the best potential by eye, we can construct a function to quantify how good a trial potential is. Let <var>x_1</var>, <var>v_1</var>, <var>\ldots</var>, <var>x_n</var>, <var>v_n</var> be a set of <var>n</var> observed positions and velocities. Then the <em>likelihood</em> that these observations came from a particular potential <var>\Phi</var> is
					<code>
						L(\Phi \,|\, x_1,v_1,\ldots,x_n,v_n) = \prod_i f(x_i, v_i \,|\, \Phi),
					</code>
					where we've defined
					<code>
						f(x,v \,|\, \Phi) = \frac{1}{T}\int_0^T\sum_i K(x_i(t) - x)\,K(v_i(t) - v) \,dt,
					</code>
					where <var>x_i(t)</var> and <var>v_i(t)</var> denote the position and velocity of the <var>i</var>th particle evolved to time <var>t</var> under the potential <var>\Phi</var>, and <var>K(\cdot)</var> is a <em>kernel function</em>, e.g., a Gaussian centered at <var>0</var>. This function <var>L</var> is greatest for potentials that preserve the original configuration.
				</p>
			</article>
			
			<article>
				<h2>Summary of the algorithm</h2>
				
				<p>Our algorithm to infer the potential from a set of observed positions and velocities is as follows.</p>
				
				<ol>
					<li><em>Guess a trial potential</em> by choosing a set of values for the parameters of a potential. We choose parameters using the Metropolis-Hastings algorithm.</li>
					<li><em>Calculate the likelihood</em> of observing the data under the chosen potential, according to the formula in the previous section.</li>
					<li><em>Repeat</em> steps 1&ndash;2 until convergence of the likelihood. The most likely potential has the parameters with the greatest likelihood.</li>
				</ol>
			</article>
			
			<article>
				<h2>A two-dimensional test case</h2>
				
				<figure class="float" style="width: 400px; margin-top: 0.75em; margin-right: -0.3em; margin-left: 0.5em">
					<img src="images/logarithmic.svg" />
					<figcaption>A contour plot of the likelihood of the parameters. Red is higher; contours are at <var>2^{-2^i}</var> for <var>i=0,\ldots,13</var>.</figcaption>
				</figure>
				
				<p>We have tested the algorithm for the two-dimensional <em>logarithmic potential</em> given by
					<code>\Phi(x,y) = \log(x^2 + \frac{y^2}{q^2} + R_c^2),</code>					
					where <var>q</var> and <var>R_c</var> are parameters that describe the shape of the potential. It is a simple model for real galaxies.</p>
				
				
				<!--figure style="width: 400px">
					<video controls loop style="width: 100%">
						<source src="images/logarithmic_potential.webm" type="video/webm" />
					</video>
					<figcaption>"Stars" in a logarithmic potential.</figcaption>
				</figure-->
				
				<p>We simulated <var>n = 10^4</var> particles in the logarithmic potential with true parameters of <var>q^2=0.8</var> and <var>R_c^2=1.5</var>. Figure 4 depicts the likelihood we calculated. It peaks near the true values of the parameters, indicating the algorithm was successful.</p>
			</article>
			
			<article>
				<h2>Advantages and future work</h2>
				
				<p>Our method has two novel advantages over existing methods <span class="cite">(e.g., Bovy et al. 2010, Magorrian 2014, Han et al. 2015)</span>:
					<ul>
						<li><b>General:</b> This algorithm requires only that the particles are in steady-state, whereas some other methods require also that the potential be integrable.</li>
						<li><b>Intuitive:</b> It has an intuitive interpretation in terms of phase-mixing.</li>
					</ul>
				</p>
				
				<p>
					Nevertheless, there are several challenges that should be addressed before applying the algorithm to the <i>Gaia</i> data:
					<ul>
						<li><b>Computation time:</b> An application to <var>10^9</var> particles will be computationally expensive.</li>
						<li><b>Noise:</b> The likelihood function is currently quite noisy, which may prevent accurate inference of parameters.</li>
						<li><b>Lack of error bounds:</b> It's currently unclear how to accurately quantify the uncertainty in the inferred parameters.</li>
					</ul>
				</p>
			</article>
			
			<article>
				<p style="font-size: 0.8em; line-height: 1.25">This material is based upon work supported by the National Science Foundation under Grant No. AST-1359462, a Research Experiences for Undergraduates (REU) grant awarded to CIERA at Northwestern University. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p>
			</article>
			
			<!--article>
				<h2>References</h2>
				
				<ol>
					<li>Gaia</li>
					<li>Binney and Tremaine</li>
					<li>Han</li>
					<li>Magorrian</li>
					<li>Bovy</li>
				</ol>
			</article-->
		</main>
		
		<!--figcaption style="position: absolute; right: 6em; bottom: 20em; color: rgba(255, 255, 255, 0.9); width: 600px; text-align: center">
			A star map generated from preliminary Gaia data. Source: ESA/Gaia &ndash; CC BY-SA 3.0 IGO.
		</figcaption-->
		
		<footer style="font-size: 0.5em; text-align: center; color: rgba(255, 255, 255, 0.75); position: absolute; bottom: 0; width: 100%; margin: 2em 0;">
			
		</footer>
		
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.3.0/katex.min.js"></script>
		<script type="text/javascript">
			[].forEach.call(document.querySelectorAll('var, code'), function (el) {
				katex.render(el.innerHTML, el, { displayMode: el.tagName === 'CODE' });
			});
		</script>
		<script type="text/javascript" src="http://smartquotesjs.com/src/smartquotes.min.js"></script>
	</body>
</html>